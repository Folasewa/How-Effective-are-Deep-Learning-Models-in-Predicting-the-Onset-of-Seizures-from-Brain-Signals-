{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca1475e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import pyedflib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt, iirnotch, sosfilt, sosfiltfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score,  precision_recall_fscore_support\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5ed99",
   "metadata": {},
   "source": [
    "First extract the seizure files from the summary.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da82dd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seizure extraction complete!\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb03_01_362_414_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb03_02_731_796_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb03_03_432_501_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb03_04_2162_2214_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb03_34_1982_2029_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb03_35_2592_2656_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb03_36_1725_1778_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_01_480_505_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_01_2451_2476_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_03_231_260_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_03_2883_2908_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_04_1088_1120_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_04_1411_1438_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_04_1745_1764_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_06_1229_1253_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_07_38_60_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_09_1745_1764_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_11_3527_3597_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_13_3288_3304_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_14_1939_1966_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_15_3552_3569_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_17_3515_3581_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb24_21_2804_2872_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb05_06_417_532_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb05_13_1086_1196_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb05_17_2451_2571_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb05_22_2348_2465_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb02_16_130_212_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb02_16+_2972_3053_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb02_19_3369_3378_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb01_03_2996_3036_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb01_04_1467_1494_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb01_15_1732_1772_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb01_16_1015_1066_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb01_18_1720_1810_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb01_21_327_420_seizure.npy\n",
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/seizure_files/chb01_26_1862_1963_seizure.npy\n"
     ]
    }
   ],
   "source": [
    "edf_path = \"/Users/folasewaabdulsalam/Seizure_Onset/edf_files\"\n",
    "summary_path= \"/Users/folasewaabdulsalam/Seizure_Onset/summary_files\"\n",
    "output_path = \"/Users/folasewaabdulsalam/Seizure_Onset/seizure_files\"\n",
    "os.makedirs(output_path , exist_ok=True)\n",
    "\n",
    "def extract_seizure_segments(edf_path, summary_path, output_path):\n",
    "    # Process each summary file\n",
    "    for summary_file in os.listdir(summary_path):\n",
    "        if summary_file.endswith(\".txt\"):\n",
    "            participant_id = summary_file.replace(\"-summary.txt\", \"\")\n",
    "            \n",
    "            # Read the summary file\n",
    "            with open(os.path.join(summary_path, summary_file), \"r\") as f:\n",
    "                current_file = None\n",
    "                seizure_times = {}\n",
    "                \n",
    "                for line in f:\n",
    "                    # Detect file names\n",
    "                    if line.startswith(\"File Name:\"):\n",
    "                        current_file = line.split(\":\")[1].strip().replace(\".edf\", \"\")\n",
    "                        seizure_times[current_file] = []\n",
    "\n",
    "                    # Detect seizure start and end times\n",
    "                    if \"Seizure Start Time:\" in line:\n",
    "                        start_time = int(line.split(\":\")[1].strip().replace(\" seconds\", \"\"))\n",
    "                        seizure_times[current_file].append((\"start\", start_time))\n",
    "\n",
    "                    if \"Seizure End Time:\" in line:\n",
    "                        end_time = int(line.split(\":\")[1].strip().replace(\" seconds\", \"\"))\n",
    "                        seizure_times[current_file].append((\"end\", end_time))\n",
    "\n",
    "                # Extract and save seizure segments\n",
    "                for file_name, events in seizure_times.items():\n",
    "                    edf_file = f\"{file_name}.edf\"\n",
    "                    edf_file_path = os.path.join(edf_path, edf_file)\n",
    "\n",
    "                    if os.path.exists(edf_file_path):\n",
    "                        with pyedflib.EdfReader(edf_file_path) as f:\n",
    "                            signals = np.array([f.readSignal(i) for i in range(f.signals_in_file)])\n",
    "                            fs = f.getSampleFrequency(0)\n",
    "\n",
    "                            # Extract seizure segments\n",
    "                            starts = [t[1] for t in events if t[0] == \"start\"]\n",
    "                            ends = [t[1] for t in events if t[0] == \"end\"]\n",
    "\n",
    "                            for start, end in zip(starts, ends):\n",
    "                                start_sample = int(start * fs)\n",
    "                                end_sample = int(end * fs)\n",
    "                                seizure_segment = signals[:, start_sample:end_sample]\n",
    "\n",
    "                                # Save the seizure segment\n",
    "                                output_file = os.path.join(output_path, f\"{file_name}_{start}_{end}_seizure.npy\")\n",
    "                                np.save(output_file, seizure_segment)\n",
    "                                print(f\"Saved {output_file}\")\n",
    "\n",
    "print(\"Seizure extraction complete!\")\n",
    "\n",
    "\n",
    "# Run the extraction\n",
    "extract_seizure_segments(edf_path, summary_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93a809",
   "metadata": {},
   "source": [
    "Convert to .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "363ed92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting full .edf files...\n",
      "Converted chb24_03.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_03.h5\n",
      "Converted chb24_17.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_17.h5\n",
      "Converted chb24_16.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_16.h5\n",
      "Converted chb24_02.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_02.h5\n",
      "Converted chb01_43.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_43.h5\n",
      "Converted chb24_14.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_14.h5\n",
      "Converted chb23_09.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_09.h5\n",
      "Converted chb23_20.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_20.h5\n",
      "Converted chb24_01.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_01.h5\n",
      "Converted chb23_08.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_08.h5\n",
      "Converted chb24_15.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_15.h5\n",
      "Converted chb24_11.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_11.h5\n",
      "Converted chb24_05.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_05.h5\n",
      "Converted chb05_34.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_34.h5\n",
      "Converted chb24_04.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_04.h5\n",
      "Converted chb23_19.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_19.h5\n",
      "Converted chb24_06.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_06.h5\n",
      "Converted chb24_12.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_12.h5\n",
      "Converted chb02_16.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_16.h5\n",
      "Converted chb05_22.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_22.h5\n",
      "Converted chb05_36.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_36.h5\n",
      "Converted chb24_13.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_13.h5\n",
      "Converted chb24_07.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_07.h5\n",
      "Converted chb01_46.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_46.h5\n",
      "Converted chb01_21.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_21.h5\n",
      "Converted chb01_08.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_08.h5\n",
      "Converted chb03_36.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb03_36.h5\n",
      "Converted chb01_26.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_26.h5\n",
      "Converted chb03_35.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb03_35.h5\n",
      "Converted chb01_18.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_18.h5\n",
      "Converted chb03_34.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb03_34.h5\n",
      "Converted chb03_05.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb03_05.h5\n",
      "Converted chb01_01.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_01.h5\n",
      "Converted chb01_15.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_15.h5\n",
      "Converted chb03_04.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb03_04.h5\n",
      "Converted chb01_03.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_03.h5\n",
      "Converted chb01_16.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_16.h5\n",
      "Converted chb01_02.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_02.h5\n",
      "Converted chb03_03.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb03_03.h5\n",
      "Converted chb01_06.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_06.h5\n",
      "Converted chb01_07.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_07.h5\n",
      "Converted chb03_02.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb03_02.h5\n",
      "Converted chb01_05.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_05.h5\n",
      "Converted chb01_38.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_38.h5\n",
      "Converted chb01_04.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_04.h5\n",
      "Converted chb01_10.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb01_10.h5\n",
      "Converted chb03_01.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb03_01.h5\n",
      "Converted chb24_22.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_22.h5\n",
      "Converted chb23_17.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_17.h5\n",
      "Converted chb02_32.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_32.h5\n",
      "Converted chb05_13.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_13.h5\n",
      "Converted chb05_06.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_06.h5\n",
      "Converted chb02_33.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_33.h5\n",
      "Converted chb02_27.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_27.h5\n",
      "Converted chb23_16.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_16.h5\n",
      "Converted chb24_21.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_21.h5\n",
      "Converted chb24_09.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_09.h5\n",
      "Converted chb02_31.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_31.h5\n",
      "Converted chb05_38.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_38.h5\n",
      "Converted chb02_19.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_19.h5\n",
      "Converted chb05_39.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_39.h5\n",
      "Converted chb24_08.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_08.h5\n",
      "Converted chb24_20.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_20.h5\n",
      "Converted chb02_16+.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_16+.h5\n",
      "Converted chb24_18.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_18.h5\n",
      "Converted chb05_01.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_01.h5\n",
      "Converted chb02_34.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_34.h5\n",
      "Converted chb02_35.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb02_35.h5\n",
      "Converted chb23_10.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_10.h5\n",
      "Converted chb24_19.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb24_19.h5\n",
      "Converted chb23_06.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_06.h5\n",
      "Converted chb05_17.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb05_17.h5\n",
      "Converted chb23_07.edf to /Users/folasewaabdulsalam/Seizure_Onset/h5_files/chb23_07.h5\n"
     ]
    }
   ],
   "source": [
    "#convert full files to .h5\n",
    "edf_path = \"/Users/folasewaabdulsalam/Seizure_Onset/edf_files\"\n",
    "h5_path= \"/Users/folasewaabdulsalam/Seizure_Onset/h5_files\"\n",
    "os.makedirs(h5_path, exist_ok=True)\n",
    "\n",
    "def convert_edf_to_h5(edf_path, h5_path):\n",
    "    for edf_file in os.listdir(edf_path):\n",
    "        if edf_file.endswith(\".edf\"):\n",
    "            edf_file_path = os.path.join(edf_path, edf_file)\n",
    "            h5_file_path = os.path.join(h5_path, edf_file.replace(\".edf\", \".h5\"))\n",
    "            \n",
    "            with pyedflib.EdfReader(edf_file_path) as f:\n",
    "                signals = np.array([f.readSignal(i) for i in range(f.signals_in_file)])\n",
    "                channels = f.getSignalLabels()\n",
    "                sample_rate = f.getSampleFrequency(0)\n",
    "\n",
    "            # Save as .h5\n",
    "            with h5py.File(h5_file_path, \"w\") as hf:\n",
    "                hf.create_dataset(\"data\", data=signals, compression=\"gzip\")\n",
    "                hf.attrs[\"channels\"] = channels\n",
    "                hf.attrs[\"sample_rate\"] = sample_rate\n",
    "            \n",
    "            print(f\"Converted {edf_file} to {h5_file_path}\")\n",
    "\n",
    "print(\"Converting full .edf files...\")\n",
    "convert_edf_to_h5(edf_path, h5_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b72ed3",
   "metadata": {},
   "source": [
    "***startttttt****\n",
    "\n",
    "Preprocessing seziures and non seizures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9388cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations saved to /Users/folasewaabdulsalam/Seizure_Onset/seizure_annotations.txt\n"
     ]
    }
   ],
   "source": [
    "#extracting seizure annotations from summary.txt\n",
    "summary_path = \"/Users/folasewaabdulsalam/Seizure_Onset/summary_files\"\n",
    "annotations_file = \"/Users/folasewaabdulsalam/Seizure_Onset/seizure_annotations.txt\"\n",
    "\n",
    "def extract_seizure_annotations(summary_path, output_file):\n",
    "    seizure_annotations = {}\n",
    "\n",
    "    for summary_file in os.listdir(summary_path):\n",
    "        if summary_file.endswith(\".txt\"):\n",
    "            with open(os.path.join(summary_path, summary_file), \"r\") as f:\n",
    "                current_file = None\n",
    "                for line in f:\n",
    "                    # Detect file names\n",
    "                    if line.startswith(\"File Name:\"):\n",
    "                        current_file = line.split(\":\")[1].strip().replace(\".edf\", \"\")\n",
    "                        seizure_annotations[current_file] = []\n",
    "\n",
    "                    # Detect seizure start and end times\n",
    "                    if \"Seizure Start Time:\" in line:\n",
    "                        start_time = int(line.split(\":\")[1].strip().replace(\" seconds\", \"\"))\n",
    "                        seizure_annotations[current_file].append((\"start\", start_time))\n",
    "\n",
    "                    if \"Seizure End Time:\" in line:\n",
    "                        end_time = int(line.split(\":\")[1].strip().replace(\" seconds\", \"\"))\n",
    "                        seizure_annotations[current_file].append((\"end\", end_time))\n",
    "\n",
    "    # Save the annotations as a single file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for file, events in seizure_annotations.items():\n",
    "            for event in events:\n",
    "                f.write(f\"{file} {event[0]} {event[1]}\\n\")\n",
    "    \n",
    "    print(f\"Annotations saved to {output_file}\")\n",
    "\n",
    "# Run the extraction\n",
    "extract_seizure_annotations(summary_path, annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42152f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:   1%|▏         | 1/73 [00:04<05:40,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_36_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:   3%|▎         | 2/73 [00:14<09:14,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_06_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:   4%|▍         | 3/73 [00:34<15:15, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_16_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:   5%|▌         | 4/73 [00:38<11:04,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_03_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:   7%|▋         | 5/73 [00:42<08:45,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_19_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:   8%|▊         | 6/73 [00:46<07:18,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_46_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  10%|▉         | 7/73 [00:51<06:22,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb03_03_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  11%|█         | 8/73 [00:55<05:45,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_09_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  12%|█▏        | 9/73 [01:00<05:24,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_06_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  14%|█▎        | 10/73 [01:04<05:05,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_22_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  15%|█▌        | 11/73 [01:08<04:52,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_19_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  16%|█▋        | 12/73 [01:13<04:38,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_07_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  18%|█▊        | 13/73 [01:17<04:27,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb03_02_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  19%|█▉        | 14/73 [01:21<04:19,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_16_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  21%|██        | 15/73 [01:25<04:13,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_08_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  22%|██▏       | 16/73 [01:30<04:07,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb03_36_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  23%|██▎       | 17/73 [01:34<04:03,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_17_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  25%|██▍       | 18/73 [01:38<03:58,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_18_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  26%|██▌       | 19/73 [01:43<03:53,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_06_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  27%|██▋       | 20/73 [01:45<03:25,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_26_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  29%|██▉       | 21/73 [01:50<03:26,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_43_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  30%|███       | 22/73 [01:53<03:11,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_07_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  32%|███▏      | 23/73 [01:57<03:16,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_13_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  33%|███▎      | 24/73 [02:13<06:13,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_17_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  34%|███▍      | 25/73 [02:18<05:17,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_02_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  36%|███▌      | 26/73 [02:22<04:38,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_07_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  37%|███▋      | 27/73 [02:26<04:09,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_17_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  38%|███▊      | 28/73 [02:31<03:49,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_27_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  40%|███▉      | 29/73 [02:35<03:35,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_16+_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  41%|████      | 30/73 [02:39<03:23,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_38_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  42%|████▏     | 31/73 [02:52<05:03,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_08_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  44%|████▍     | 32/73 [02:57<04:20,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_33_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  45%|████▌     | 33/73 [03:01<03:51,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_03_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  47%|████▋     | 34/73 [03:05<03:28,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_13_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  48%|████▊     | 35/73 [03:10<03:11,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_39_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  49%|████▉     | 36/73 [03:28<05:37,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_09_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  51%|█████     | 37/73 [03:33<04:35,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_32_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  52%|█████▏    | 38/73 [03:37<03:53,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_02_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  53%|█████▎    | 39/73 [03:41<03:22,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_12_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  55%|█████▍    | 40/73 [03:46<03:00,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_38_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  56%|█████▌    | 41/73 [03:47<02:16,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_16_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  58%|█████▊    | 42/73 [04:07<04:36,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_19_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  59%|█████▉    | 43/73 [04:11<03:46,  7.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_06_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  60%|██████    | 44/73 [04:15<03:10,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_18_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  62%|██████▏   | 45/73 [04:17<02:19,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_22_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  63%|██████▎   | 46/73 [04:21<02:08,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_08_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  64%|██████▍   | 47/73 [04:25<02:01,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_16_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  66%|██████▌   | 48/73 [04:30<01:53,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_31_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  67%|██████▋   | 49/73 [04:34<01:47,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_01_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  68%|██████▊   | 50/73 [04:38<01:41,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_11_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  70%|██████▉   | 51/73 [04:43<01:37,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_35_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  71%|███████   | 52/73 [04:47<01:32,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_05_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  73%|███████▎  | 53/73 [04:52<01:29,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_21_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  74%|███████▍  | 54/73 [04:56<01:24,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_15_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  75%|███████▌  | 55/73 [05:00<01:19,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb02_34_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  77%|███████▋  | 56/73 [05:05<01:14,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_04_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  78%|███████▊  | 57/73 [05:09<01:10,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_20_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  79%|███████▉  | 58/73 [05:13<01:05,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb24_14_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  81%|████████  | 59/73 [05:18<01:00,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_15_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  82%|████████▏ | 60/73 [05:22<00:55,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb03_01_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  84%|████████▎ | 61/73 [05:26<00:51,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb03_35_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  85%|████████▍ | 62/73 [05:31<00:47,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_21_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  86%|████████▋ | 63/73 [05:49<01:25,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_10_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  88%|████████▊ | 64/73 [05:53<01:05,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_05_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  89%|████████▉ | 65/73 [05:57<00:50,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb03_05_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  90%|█████████ | 66/73 [06:02<00:40,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_34_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  92%|█████████▏| 67/73 [06:08<00:34,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb23_20_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  93%|█████████▎| 68/73 [06:12<00:26,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_01_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  95%|█████████▍| 69/73 [06:16<00:19,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb03_04_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  96%|█████████▌| 70/73 [06:20<00:14,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_10_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  97%|█████████▋| 71/73 [06:25<00:09,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb05_01_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files:  99%|█████████▊| 72/73 [06:29<00:04,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb03_34_preprocessed.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing .h5 Files: 100%|██████████| 73/73 [06:33<00:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data/chb01_04_preprocessed.h5\n",
      "\n",
      "Preprocessing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directories\n",
    "h5_full_path = \"/Users/folasewaabdulsalam/Seizure_Onset/h5_files\"\n",
    "annotations_file = \"/Users/folasewaabdulsalam/Seizure_Onset/seizure_annotations.txt\"\n",
    "preprocessed_path = \"/Users/folasewaabdulsalam/Seizure_Onset/preprocessed_data\"\n",
    "os.makedirs(preprocessed_path, exist_ok=True)\n",
    "\n",
    "FILTER_BANDS = [\n",
    "    (0.5, 3.625),\n",
    "    (3.625, 6.75),\n",
    "    (6.75, 9.875),\n",
    "    (9.875, 13.0),\n",
    "    (13.0, 16.125),\n",
    "    (16.125, 19.25),\n",
    "    (19.25, 22.375),\n",
    "    (22.375, 25.0)\n",
    "]\n",
    "\n",
    "SELECTED_CHANNELS = list(range(18))\n",
    "\n",
    "WINDOW_SIZE = 2 * 256  # 2 seconds at 256 Hz\n",
    "FS = 256  # Sampling rate\n",
    "\n",
    "def load_annotations(annotations_file):\n",
    "    \"\"\"\n",
    "    Load seizure annotations from a plain text file.\n",
    "    \"\"\"\n",
    "    seizure_annotations = {}\n",
    "    with open(annotations_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Split by space\n",
    "            parts = line.strip().split()\n",
    "            \n",
    "            # Extract file name (without the _preprocessed suffix)\n",
    "            file_name = parts[0].strip()\n",
    "            \n",
    "            # Extract event and time\n",
    "            event = parts[1].strip().encode(\"utf-8\")\n",
    "            time = int(parts[2].strip())\n",
    "            \n",
    "            # Store the annotation\n",
    "            if file_name not in seizure_annotations:\n",
    "                seizure_annotations[file_name] = []\n",
    "            seizure_annotations[file_name].append((event, time))\n",
    "    \n",
    "    # Convert each list to a structured numpy array\n",
    "    for key in seizure_annotations.keys():\n",
    "        seizure_annotations[key] = np.array(seizure_annotations[key], dtype=[(\"event\", \"S5\"), (\"time\", \"i4\")])\n",
    "    \n",
    "    return seizure_annotations\n",
    "\n",
    "def preprocess_h5_files(h5_full_path, preprocessed_path, annotations_file):\n",
    "    \"\"\"\n",
    "    Preprocess .h5 files by applying bandpass filtering and channel selection.\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    seizure_annotations = load_annotations(annotations_file)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(h5_full_path), desc=\"Preprocessing .h5 Files\"):\n",
    "        if file_name.endswith(\".h5\"):\n",
    "            file_path = os.path.join(h5_full_path, file_name)\n",
    "            output_file_path = os.path.join(preprocessed_path, file_name.replace(\".h5\", \"_preprocessed.h5\"))\n",
    "\n",
    "            with h5py.File(file_path, \"r\") as hf:\n",
    "                signals = hf[\"data\"][:]\n",
    "\n",
    "                # Select only the first 18 channels\n",
    "                if signals.shape[0] < 18:\n",
    "                    print(f\"⚠️ WARNING: Skipping file {file_name} with fewer than 18 channels. Found {signals.shape[0]}\")\n",
    "                    continue\n",
    "\n",
    "                signals = signals[:18, :]  # Use the first 18 channels\n",
    "\n",
    "                # Apply bandpass filtering\n",
    "                sos = butter(4, [0.5, 25.0], btype='band', fs=FS, output='sos')\n",
    "                filtered_signals = sosfiltfilt(sos, signals, axis=1)\n",
    "\n",
    "                # Save the preprocessed data\n",
    "                with h5py.File(output_file_path, \"w\") as out_hf:\n",
    "                    out_hf.create_dataset(\"data\", data=filtered_signals, compression=\"gzip\")\n",
    "                    out_hf.attrs[\"channels\"] = list(range(18))\n",
    "\n",
    "                    # Attach annotations if available\n",
    "                    file_base_name = file_name.replace(\".h5\", \"\")\n",
    "                    if file_base_name in seizure_annotations:\n",
    "                        out_hf.attrs[\"seizure_annotations\"] = seizure_annotations[file_base_name]\n",
    "\n",
    "                print(f\"Saved {output_file_path}\")\n",
    "\n",
    "preprocess_h5_files(h5_full_path, preprocessed_path, annotations_file)\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eb303ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   1%|▏         | 1/73 [00:05<06:08,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_46_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   3%|▎         | 2/73 [00:09<05:52,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_01_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   4%|▍         | 3/73 [00:15<05:55,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_07_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   5%|▌         | 4/73 [00:20<05:54,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_18_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   7%|▋         | 5/73 [00:25<05:57,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_11_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   8%|▊         | 6/73 [00:31<05:51,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_34_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  10%|▉         | 7/73 [00:36<05:45,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_08_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  11%|█         | 8/73 [00:41<05:43,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb03_03_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  12%|█▏        | 9/73 [00:46<05:34,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb03_04_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  14%|█▎        | 10/73 [00:52<05:31,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_13_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  15%|█▌        | 11/73 [00:57<05:29,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_33_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  16%|█▋        | 12/73 [01:02<05:20,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_16_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  18%|█▊        | 13/73 [01:07<05:05,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_10_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  19%|█▉        | 14/73 [01:12<04:54,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_38_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  21%|██        | 15/73 [01:17<04:55,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_19_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  22%|██▏       | 16/73 [01:24<05:22,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_20_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  23%|██▎       | 17/73 [01:29<05:03,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb03_36_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  25%|██▍       | 18/73 [01:34<04:49,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_06_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  26%|██▌       | 19/73 [01:38<04:35,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_32_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  27%|██▋       | 20/73 [01:43<04:27,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb03_05_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  29%|██▉       | 21/73 [01:48<04:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_08_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  30%|███       | 22/73 [01:53<04:16,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_17_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  32%|███▏      | 23/73 [01:55<03:15,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_16_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  33%|███▎      | 24/73 [01:59<03:23,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_36_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  34%|███▍      | 25/73 [02:04<03:29,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_18_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  36%|███▌      | 26/73 [02:09<03:34,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_39_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  37%|███▋      | 27/73 [02:14<03:35,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_19_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  38%|███▊      | 28/73 [02:19<03:32,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_01_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  40%|███▉      | 29/73 [02:24<03:29,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_07_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  41%|████      | 30/73 [02:29<03:26,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_06_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  42%|████▏     | 31/73 [02:30<02:39,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_22_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  44%|████▍     | 32/73 [02:35<02:49,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_16+_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  45%|████▌     | 33/73 [02:40<02:54,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_16_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  47%|████▋     | 34/73 [02:45<02:55,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_09_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  48%|████▊     | 35/73 [02:50<02:54,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb03_02_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  49%|████▉     | 36/73 [02:55<02:52,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_35_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  51%|█████     | 37/73 [03:09<04:30,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_08_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  52%|█████▏    | 38/73 [03:13<03:54,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_15_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  53%|█████▎    | 39/73 [03:18<03:29,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_34_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  55%|█████▍    | 40/73 [03:23<03:12,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_22_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  56%|█████▌    | 41/73 [03:29<02:59,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_03_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  58%|█████▊    | 42/73 [03:33<02:47,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_38_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  59%|█████▉    | 43/73 [03:38<02:35,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_05_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  60%|██████    | 44/73 [03:43<02:27,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb03_35_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  62%|██████▏   | 45/73 [03:48<02:19,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_21_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  63%|██████▎   | 46/73 [03:51<02:01,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_07_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  64%|██████▍   | 47/73 [03:56<01:58,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_06_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  66%|██████▌   | 48/73 [04:01<01:57,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_01_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  67%|██████▋   | 49/73 [04:06<01:58,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_20_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  68%|██████▊   | 50/73 [04:10<01:42,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_26_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  70%|██████▉   | 51/73 [04:15<01:41,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_02_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  71%|███████   | 52/73 [04:35<03:16,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_19_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  73%|███████▎  | 53/73 [04:40<02:40,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_04_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  74%|███████▍  | 54/73 [05:00<03:39, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_16_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  75%|███████▌  | 55/73 [05:05<02:52,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb05_17_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  77%|███████▋  | 56/73 [05:10<02:18,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_12_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  78%|███████▊  | 57/73 [05:14<01:54,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_21_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  79%|███████▉  | 58/73 [05:19<01:37,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_03_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  81%|████████  | 59/73 [05:24<01:24,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_05_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  82%|████████▏ | 60/73 [05:41<02:00,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_17_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  84%|████████▎ | 61/73 [05:46<01:35,  7.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb03_01_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  85%|████████▍ | 62/73 [05:51<01:17,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_15_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  86%|████████▋ | 63/73 [05:56<01:03,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_13_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  88%|████████▊ | 64/73 [06:01<00:53,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_14_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  89%|████████▉ | 65/73 [06:20<01:19,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_09_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  90%|█████████ | 66/73 [06:39<01:28, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_10_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  92%|█████████▏| 67/73 [06:44<01:02, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_31_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  93%|█████████▎| 68/73 [06:49<00:43,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb24_02_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  95%|█████████▍| 69/73 [06:54<00:30,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_04_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  96%|█████████▌| 70/73 [06:58<00:19,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb03_34_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  97%|█████████▋| 71/73 [07:03<00:12,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb01_43_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:  99%|█████████▊| 72/73 [07:08<00:05,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb02_27_features.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 73/73 [07:18<00:00,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/folasewaabdulsalam/Seizure_Onset/feature_path/chb23_06_features.h5\n",
      "\n",
      "Feature extraction complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#extracting features\n",
    "feature_path = \"/Users/folasewaabdulsalam/Seizure_Onset/feature_path\"\n",
    "os.makedirs(feature_path, exist_ok=True)\n",
    "\n",
    "def extract_spectral_energy(data, fs=FS):\n",
    "    \"\"\"\n",
    "    Extracts spectral energy features for each 2-second window.\n",
    "    \"\"\"\n",
    "    num_channels = data.shape[0]\n",
    "    num_samples = data.shape[1]\n",
    "    num_windows = (num_samples - WINDOW_SIZE) // WINDOW_SIZE + 1\n",
    "\n",
    "    feature_vectors = []\n",
    "\n",
    "    for i in range(num_windows):\n",
    "        start = i * WINDOW_SIZE\n",
    "        end = start + WINDOW_SIZE\n",
    "        window = data[:, start:end]\n",
    "\n",
    "        # Extract 8-band energy per channel\n",
    "        features = []\n",
    "        for low_cut, high_cut in FILTER_BANDS:\n",
    "            nyquist = 0.5 * fs\n",
    "            low = low_cut / nyquist\n",
    "            high = high_cut / nyquist\n",
    "            b, a = butter(4, [low, high], btype='band')\n",
    "            filtered = filtfilt(b, a, window, axis=1)\n",
    "            \n",
    "            # Calculate energy (sum of squares)\n",
    "            energy = np.sum(filtered ** 2, axis=1)  # Shape: (18,)\n",
    "            features.append(energy)\n",
    "\n",
    "        # Flatten to create the final feature vector (8 bands × 18 channels = 144)\n",
    "        feature_vector = np.concatenate(features)\n",
    "        feature_vectors.append(feature_vector)\n",
    "\n",
    "    return np.array(feature_vectors)\n",
    "\n",
    "def extract_and_save_features(preprocessed_path, feature_path, annotations_file):\n",
    "    \"\"\"\n",
    "    Extract features from all preprocessed files.\n",
    "    \"\"\"\n",
    "    # Load annotations\n",
    "    seizure_annotations = load_annotations(annotations_file)\n",
    "\n",
    "    # Get all preprocessed .h5 files\n",
    "    h5_files = [f for f in os.listdir(preprocessed_path) if f.endswith(\"_preprocessed.h5\")]\n",
    "    \n",
    "    for h5_file in tqdm(h5_files, desc=\"Extracting Features\"):\n",
    "        file_path = os.path.join(preprocessed_path, h5_file)\n",
    "        output_file = os.path.join(feature_path, h5_file.replace(\"_preprocessed.h5\", \"_features.h5\"))\n",
    "        file_base_name = h5_file.replace(\"_preprocessed.h5\", \"\")\n",
    "\n",
    "        # Load the preprocessed data\n",
    "        with h5py.File(file_path, \"r\") as hf:\n",
    "            signals = hf[\"data\"][:]\n",
    "            # Use only the first 18 channels\n",
    "            signals = signals[SELECTED_CHANNELS, :]\n",
    "            features = extract_spectral_energy(signals)\n",
    "        \n",
    "        # Extract and add annotations\n",
    "        labels = np.zeros(features.shape[0], dtype=int)  # Default to non-seizure\n",
    "        if file_base_name in seizure_annotations:\n",
    "            for event, time in seizure_annotations[file_base_name]:\n",
    "                if event == b\"start\":\n",
    "                    start_idx = time // 2\n",
    "                elif event == b\"end\":\n",
    "                    end_idx = time // 2\n",
    "                    labels[start_idx:end_idx] = 1  # Mark seizure windows\n",
    "        \n",
    "        # Save the features and labels\n",
    "        with h5py.File(output_file, \"w\") as out_hf:\n",
    "            out_hf.create_dataset(\"features\", data=features, compression=\"gzip\")\n",
    "            out_hf.create_dataset(\"labels\", data=labels, compression=\"gzip\")\n",
    "            out_hf.attrs[\"channels\"] = SELECTED_CHANNELS\n",
    "            if file_base_name in seizure_annotations:\n",
    "                out_hf.attrs[\"seizure_annotations\"] = seizure_annotations[file_base_name]\n",
    "        \n",
    "        print(f\"Saved {output_file}\")\n",
    "\n",
    "# Run the feature extraction\n",
    "extract_and_save_features(preprocessed_path, feature_path, annotations_file)\n",
    "\n",
    "print(\"\\nFeature extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "254941d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading features and labels with the annotations\n",
    "from collections import Counter\n",
    "def load_features_and_labels(feature_path, annotations_file):\n",
    "    \"\"\"\n",
    "    Load features and labels from all feature files.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Load annotations\n",
    "    seizure_annotations = {}\n",
    "    with open(annotations_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            file_name = parts[0].strip()\n",
    "            event_type = parts[1].strip().encode(\"utf-8\")\n",
    "            event_time = int(parts[2].strip())\n",
    "            \n",
    "            if file_name not in seizure_annotations:\n",
    "                seizure_annotations[file_name] = []\n",
    "            seizure_annotations[file_name].append((event_type, event_time))\n",
    "\n",
    "    for file_name in os.listdir(feature_path):\n",
    "        if file_name.endswith(\"_features.h5\"):\n",
    "            file_base_name = file_name.replace(\"_features.h5\", \"\")\n",
    "            file_path = os.path.join(feature_path, file_name)\n",
    "            \n",
    "            with h5py.File(file_path, \"r\") as hf:\n",
    "                data = hf[\"features\"][:]\n",
    "                \n",
    "                # Check for seizure annotations\n",
    "                file_annotations = seizure_annotations.get(file_base_name, [])\n",
    "                labels_for_file = np.zeros(data.shape[0], dtype=int)\n",
    "\n",
    "                # Mark seizure windows\n",
    "                for event, time in file_annotations:\n",
    "                    if event == b\"start\":\n",
    "                        start_idx = time // 2\n",
    "                        labels_for_file[start_idx:] = 1\n",
    "                    elif event == b\"end\":\n",
    "                        end_idx = time // 2\n",
    "                        labels_for_file[end_idx:] = 0\n",
    "\n",
    "                features.append(data)\n",
    "                labels.append(labels_for_file)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    features = np.vstack(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    return features, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7c747e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Set Ratio: Counter({np.int64(0): 31757, np.int64(1): 200})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 127028})\n",
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      "Model: Naive Bayes\n",
      "F1 Score: 0.9814\n",
      "Sensitivity (Recall): 0.4800\n",
      "Specificity: 0.9764\n",
      "Latency: 0.4000 samples\n",
      "Confusion Matrix:\n",
      "[[31008   749]\n",
      " [  104    96]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     31757\n",
      "           1       0.11      0.48      0.18       200\n",
      "\n",
      "    accuracy                           0.97     31957\n",
      "   macro avg       0.56      0.73      0.59     31957\n",
      "weighted avg       0.99      0.97      0.98     31957\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Model: Random Forest\n",
      "F1 Score: 0.9970\n",
      "Sensitivity (Recall): 0.7200\n",
      "Specificity: 0.9988\n",
      "Latency: 1359.5519 samples\n",
      "Confusion Matrix:\n",
      "[[31718    39]\n",
      " [   56   144]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31757\n",
      "           1       0.79      0.72      0.75       200\n",
      "\n",
      "    accuracy                           1.00     31957\n",
      "   macro avg       0.89      0.86      0.88     31957\n",
      "weighted avg       1.00      1.00      1.00     31957\n",
      "\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [03:48:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: XGBoost\n",
      "F1 Score: 0.9975\n",
      "Sensitivity (Recall): 0.8600\n",
      "Specificity: 0.9983\n",
      "Latency: 4.1150 samples\n",
      "Confusion Matrix:\n",
      "[[31703    54]\n",
      " [   28   172]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31757\n",
      "           1       0.76      0.86      0.81       200\n",
      "\n",
      "    accuracy                           1.00     31957\n",
      "   macro avg       0.88      0.93      0.90     31957\n",
      "weighted avg       1.00      1.00      1.00     31957\n",
      "\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#training the model\n",
    "model_results_path = \"/Users/folasewaabdulsalam/Seizure_Onset/model_results\"\n",
    "\n",
    "# Load Data\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "# Preserve Original Seizure-to-Non-Seizure Ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print Ratio\n",
    "print(\"\\nTraining Set Ratio:\", Counter(y_train))\n",
    "print(\"Test Set Ratio:\", Counter(y_test))\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Balance Training Data\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print Balanced Ratio\n",
    "print(\"\\nBalanced Training Set Ratio:\", Counter(y_train_balanced))\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and Evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Sensitivity and Specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Latency Calculation (Onset Delay)\n",
    "    latency = np.mean([max(0, i - j) for i, j in zip(np.where(y_pred == 1)[0], np.where(y_test == 1)[0])]) if np.any(y_test == 1) else 0\n",
    "\n",
    "    # Save Results\n",
    "    with open(os.path.join(model_results_path, f\"{model_name}_results.txt\"), \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "        f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "        f.write(f\"Latency: {latency:.4f} samples\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Latency: {latency:.4f} samples\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "49ad08a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Set Ratio: Counter({np.int64(0): 31757, np.int64(1): 200})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 127028})\n",
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      "Model: Naive Bayes\n",
      "F1 Score: 0.9814\n",
      "Sensitivity (Recall): 0.4800\n",
      "Specificity: 0.9764\n",
      "Latency: 282.6888 seconds\n",
      "Confusion Matrix:\n",
      "[[31008   749]\n",
      " [  104    96]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     31757\n",
      "           1       0.11      0.48      0.18       200\n",
      "\n",
      "    accuracy                           0.97     31957\n",
      "   macro avg       0.56      0.73      0.59     31957\n",
      "weighted avg       0.99      0.97      0.98     31957\n",
      "\n",
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_balanced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_balanced\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    479\u001b[0m ]\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Seizure_Onset/venv/lib/python3.9/site-packages/joblib/parallel.py:1985\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1984\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Seizure_Onset/venv/lib/python3.9/site-packages/joblib/parallel.py:1913\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1913\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    198\u001b[0m         X,\n\u001b[1;32m    199\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    203\u001b[0m     )\n",
      "File \u001b[0;32m~/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training 2\n",
    "# Paths\n",
    "model_results_path = \"/Users/folasewaabdulsalam/Seizure_Onset/model_results\"\n",
    "os.makedirs(model_results_path, exist_ok=True)\n",
    "\n",
    "# Load Features and Labels\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "# Preserve Original Seizure-to-Non-Seizure Ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print Ratio\n",
    "print(\"\\nTraining Set Ratio:\", Counter(y_train))\n",
    "print(\"Test Set Ratio:\", Counter(y_test))\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Balance Training Data\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print Balanced Ratio\n",
    "print(\"\\nBalanced Training Set Ratio:\", Counter(y_train_balanced))\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and Evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Sensitivity and Specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Improved Latency Calculation (Onset Delay in Seconds)\n",
    "    latency = 0\n",
    "    true_onsets = np.where(y_test == 1)[0]\n",
    "    predicted_onsets = np.where(y_pred == 1)[0]\n",
    "\n",
    "    if len(true_onsets) > 0 and len(predicted_onsets) > 0:\n",
    "        latency_list = []\n",
    "        for pred in predicted_onsets:\n",
    "            # Find the closest true onset before the predicted onset\n",
    "            closest_true = true_onsets[true_onsets <= pred]\n",
    "            if len(closest_true) > 0:\n",
    "                latency_list.append((pred - closest_true[-1]) * 2)  # Convert to seconds\n",
    "\n",
    "        # Calculate the average latency if any valid matches were found\n",
    "        latency = np.mean(latency_list) if latency_list else 0\n",
    "\n",
    "    # Save Results\n",
    "    with open(os.path.join(model_results_path, f\"{model_name}_results2.txt\"), \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "        f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "        f.write(f\"Latency: {latency:.4f} seconds\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Latency: {latency:.4f} seconds\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTraining Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8b93bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Set Ratio: Counter({np.int64(0): 31757, np.int64(1): 200})\n",
      "\n",
      "Balanced Training Set Ratio: Counter({np.int64(0): 798, np.int64(1): 798})\n",
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      "Model: Naive Bayes\n",
      "F1 Score: 0.9684\n",
      "Sensitivity (Recall): 0.6250\n",
      "Specificity: 0.9509\n",
      "Latency: 293.1960 seconds\n",
      "Confusion Matrix:\n",
      "[[30197  1560]\n",
      " [   75   125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     31757\n",
      "           1       0.07      0.62      0.13       200\n",
      "\n",
      "    accuracy                           0.95     31957\n",
      "   macro avg       0.54      0.79      0.55     31957\n",
      "weighted avg       0.99      0.95      0.97     31957\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Model: Random Forest\n",
      "F1 Score: 0.9714\n",
      "Sensitivity (Recall): 0.9350\n",
      "Specificity: 0.9539\n",
      "Latency: 279.2515 seconds\n",
      "Confusion Matrix:\n",
      "[[30293  1464]\n",
      " [   13   187]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     31757\n",
      "           1       0.11      0.94      0.20       200\n",
      "\n",
      "    accuracy                           0.95     31957\n",
      "   macro avg       0.56      0.94      0.59     31957\n",
      "weighted avg       0.99      0.95      0.97     31957\n",
      "\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [03:58:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: XGBoost\n",
      "F1 Score: 0.9723\n",
      "Sensitivity (Recall): 0.9450\n",
      "Specificity: 0.9555\n",
      "Latency: 285.5241 seconds\n",
      "Confusion Matrix:\n",
      "[[30344  1413]\n",
      " [   11   189]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     31757\n",
      "           1       0.12      0.94      0.21       200\n",
      "\n",
      "    accuracy                           0.96     31957\n",
      "   macro avg       0.56      0.95      0.59     31957\n",
      "weighted avg       0.99      0.96      0.97     31957\n",
      "\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "#trainig 3\n",
    "# Load Features and Labels\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "# Preserve Original Seizure-to-Non-Seizure Ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print Initial Ratio\n",
    "print(\"\\nTraining Set Ratio:\", Counter(y_train))\n",
    "print(\"Test Set Ratio:\", Counter(y_test))\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Downsample the Majority Class\n",
    "def downsample(X, y):\n",
    "    \"\"\"\n",
    "    Downsamples the majority class to match the size of the minority class.\n",
    "    \"\"\"\n",
    "    # Separate the majority and minority classes\n",
    "    X_majority = X[y == 0]\n",
    "    y_majority = y[y == 0]\n",
    "    X_minority = X[y == 1]\n",
    "    y_minority = y[y == 1]\n",
    "\n",
    "    # Downsample the majority class\n",
    "    np.random.seed(42)\n",
    "    majority_indices = np.random.choice(len(X_majority), len(X_minority), replace=False)\n",
    "    X_majority_downsampled = X_majority[majority_indices]\n",
    "    y_majority_downsampled = y_majority[majority_indices]\n",
    "\n",
    "    # Combine the downsampled majority class with the minority class\n",
    "    X_balanced = np.vstack((X_majority_downsampled, X_minority))\n",
    "    y_balanced = np.hstack((y_majority_downsampled, y_minority))\n",
    "\n",
    "    # Shuffle the balanced data\n",
    "    indices = np.arange(len(y_balanced))\n",
    "    np.random.shuffle(indices)\n",
    "    return X_balanced[indices], y_balanced[indices]\n",
    "\n",
    "X_train_balanced, y_train_balanced = downsample(X_train, y_train)\n",
    "\n",
    "# Print Balanced Ratio\n",
    "print(\"\\nBalanced Training Set Ratio:\", Counter(y_train_balanced))\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and Evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Sensitivity and Specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Improved Latency Calculation (Onset Delay in Seconds)\n",
    "    latency = 0\n",
    "    true_onsets = np.where(y_test == 1)[0]\n",
    "    predicted_onsets = np.where(y_pred == 1)[0]\n",
    "\n",
    "    if len(true_onsets) > 0 and len(predicted_onsets) > 0:\n",
    "        latency_list = []\n",
    "        for pred in predicted_onsets:\n",
    "            # Find the closest true onset before the predicted onset\n",
    "            closest_true = true_onsets[true_onsets <= pred]\n",
    "            if len(closest_true) > 0:\n",
    "                latency_list.append((pred - closest_true[-1]) * 2)  # Convert to seconds\n",
    "\n",
    "        # Calculate the average latency if any valid matches were found\n",
    "        latency = np.mean(latency_list) if latency_list else 0\n",
    "\n",
    "    # Save Results\n",
    "    with open(os.path.join(model_results_path, f\"{model_name}_results3.txt\"), \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "        f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "        f.write(f\"Latency: {latency:.4f} seconds\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Latency: {latency:.4f} seconds\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d80b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Set Ratio: Counter({np.int64(0): 31757, np.int64(1): 200})\n",
      "\n",
      "Balanced Training Set Ratio: Counter({np.int64(0): 798, np.int64(1): 798})\n",
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      "Model: Naive Bayes\n",
      "F1 Score: 0.9684\n",
      "Sensitivity (Recall): 0.6250\n",
      "Specificity: 0.9509\n",
      "Latency: 0.0000 seconds\n",
      "Confusion Matrix:\n",
      "[[30197  1560]\n",
      " [   75   125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     31757\n",
      "           1       0.07      0.62      0.13       200\n",
      "\n",
      "    accuracy                           0.95     31957\n",
      "   macro avg       0.54      0.79      0.55     31957\n",
      "weighted avg       0.99      0.95      0.97     31957\n",
      "\n",
      "\n",
      "Training Random Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Random Forest\n",
      "F1 Score: 0.9705\n",
      "Sensitivity (Recall): 0.9350\n",
      "Specificity: 0.9524\n",
      "Latency: 0.0000 seconds\n",
      "Confusion Matrix:\n",
      "[[30244  1513]\n",
      " [   13   187]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     31757\n",
      "           1       0.11      0.94      0.20       200\n",
      "\n",
      "    accuracy                           0.95     31957\n",
      "   macro avg       0.55      0.94      0.59     31957\n",
      "weighted avg       0.99      0.95      0.97     31957\n",
      "\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [04:00:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: XGBoost\n",
      "F1 Score: 0.9723\n",
      "Sensitivity (Recall): 0.9450\n",
      "Specificity: 0.9555\n",
      "Latency: 0.0000 seconds\n",
      "Confusion Matrix:\n",
      "[[30344  1413]\n",
      " [   11   189]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     31757\n",
      "           1       0.12      0.94      0.21       200\n",
      "\n",
      "    accuracy                           0.96     31957\n",
      "   macro avg       0.56      0.95      0.59     31957\n",
      "weighted avg       0.99      0.96      0.97     31957\n",
      "\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# Load Data\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "# Preserve Original Seizure-to-Non-Seizure Ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print Ratio\n",
    "print(\"\\nTraining Set Ratio:\", Counter(y_train))\n",
    "print(\"Test Set Ratio:\", Counter(y_test))\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Balance Training Data (Downsampling)\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print Balanced Ratio\n",
    "print(\"\\nBalanced Training Set Ratio:\", Counter(y_train_balanced))\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and Evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Sensitivity and Specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "        # Latency Calculation (Onset Delay)\n",
    "    \n",
    "    latency = np.mean([max(0, i - j) for i, j in zip(np.where(y_pred == 1)[0], np.where(y_test == 1)[0])]) if np.any(y_test == 1) else 0\n",
    "    \n",
    "    # Save Results\n",
    "    with open(os.path.join(model_results_path, f\"{model_name}_results4.txt\"), \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "        f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "        f.write(f\"Latency: {latency:.4f} seconds\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Latency: {latency:.4f} seconds\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4de654d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Set Ratio: Counter({np.int64(0): 31757, np.int64(1): 200})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 127028})\n",
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      "Model: Naive Bayes\n",
      "F1 Score: 0.9814\n",
      "Sensitivity (Recall): 0.4800\n",
      "Specificity: 0.9764\n",
      "Latency: 0.4000 samples\n",
      "Confusion Matrix:\n",
      "[[31008   749]\n",
      " [  104    96]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     31757\n",
      "           1       0.11      0.48      0.18       200\n",
      "\n",
      "    accuracy                           0.97     31957\n",
      "   macro avg       0.56      0.73      0.59     31957\n",
      "weighted avg       0.99      0.97      0.98     31957\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Model: Random Forest\n",
      "F1 Score: 0.9970\n",
      "Sensitivity (Recall): 0.7200\n",
      "Specificity: 0.9988\n",
      "Latency: 1359.5519 samples\n",
      "Confusion Matrix:\n",
      "[[31718    39]\n",
      " [   56   144]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31757\n",
      "           1       0.79      0.72      0.75       200\n",
      "\n",
      "    accuracy                           1.00     31957\n",
      "   macro avg       0.89      0.86      0.88     31957\n",
      "weighted avg       1.00      1.00      1.00     31957\n",
      "\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [04:25:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: XGBoost\n",
      "F1 Score: 0.9975\n",
      "Sensitivity (Recall): 0.8600\n",
      "Specificity: 0.9983\n",
      "Latency: 4.1150 samples\n",
      "Confusion Matrix:\n",
      "[[31703    54]\n",
      " [   28   172]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31757\n",
      "           1       0.76      0.86      0.81       200\n",
      "\n",
      "    accuracy                           1.00     31957\n",
      "   macro avg       0.88      0.93      0.90     31957\n",
      "weighted avg       1.00      1.00      1.00     31957\n",
      "\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "#training 5\n",
    "# Load Data\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "# Preserve Original Seizure-to-Non-Seizure Ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print Ratio\n",
    "print(\"\\nTraining Set Ratio:\", Counter(y_train))\n",
    "print(\"Test Set Ratio:\", Counter(y_test))\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Balance Training Data\n",
    "smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print Balanced Ratio\n",
    "print(\"\\nBalanced Training Set Ratio:\", Counter(y_train_balanced))\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and Evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Sensitivity and Specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Latency Calculation (Onset Delay)\n",
    "    latency = np.mean([max(0, i - j) for i, j in zip(np.where(y_pred == 1)[0], np.where(y_test == 1)[0])]) if np.any(y_test == 1) else 0\n",
    "\n",
    "    # Save Results\n",
    "    with open(os.path.join(model_results_path, f\"{model_name}_results5.txt\"), \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "        f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "        f.write(f\"Latency: {latency:.4f} samples\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Latency: {latency:.4f} samples\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9faf98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Ratio: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Set Ratio: Counter({np.int64(0): 31757, np.int64(1): 200})\n",
      "\n",
      "Balanced Training Set Ratio: Counter({np.int64(0): 798, np.int64(1): 798})\n",
      "\n",
      "Training Naive Bayes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Naive Bayes\n",
      "F1 Score: 0.9684\n",
      "Sensitivity (Recall): 0.6250\n",
      "Specificity: 0.9509\n",
      "Latency: 0.0000 samples\n",
      "Confusion Matrix:\n",
      "[[30197  1560]\n",
      " [   75   125]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     31757\n",
      "           1       0.07      0.62      0.13       200\n",
      "\n",
      "    accuracy                           0.95     31957\n",
      "   macro avg       0.54      0.79      0.55     31957\n",
      "weighted avg       0.99      0.95      0.97     31957\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Model: Random Forest\n",
      "F1 Score: 0.9706\n",
      "Sensitivity (Recall): 0.9350\n",
      "Specificity: 0.9525\n",
      "Latency: 0.0000 samples\n",
      "Confusion Matrix:\n",
      "[[30248  1509]\n",
      " [   13   187]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     31757\n",
      "           1       0.11      0.94      0.20       200\n",
      "\n",
      "    accuracy                           0.95     31957\n",
      "   macro avg       0.55      0.94      0.59     31957\n",
      "weighted avg       0.99      0.95      0.97     31957\n",
      "\n",
      "\n",
      "Training XGBoost...\n",
      "\n",
      "Model: XGBoost\n",
      "F1 Score: 0.9725\n",
      "Sensitivity (Recall): 0.9400\n",
      "Specificity: 0.9559\n",
      "Latency: 0.0000 samples\n",
      "Confusion Matrix:\n",
      "[[30358  1399]\n",
      " [   12   188]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     31757\n",
      "           1       0.12      0.94      0.21       200\n",
      "\n",
      "    accuracy                           0.96     31957\n",
      "   macro avg       0.56      0.95      0.59     31957\n",
      "weighted avg       0.99      0.96      0.97     31957\n",
      "\n",
      "\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "#training 6 with downsample\n",
    "# Load Data\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "# Preserve Original Seizure-to-Non-Seizure Ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Print Ratio\n",
    "print(\"\\nTraining Set Ratio:\", Counter(y_train))\n",
    "print(\"Test Set Ratio:\", Counter(y_test))\n",
    "\n",
    "# Normalize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Balance Training Data with Downsampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print Balanced Ratio\n",
    "print(\"\\nBalanced Training Set Ratio:\", Counter(y_train_balanced))\n",
    "\n",
    "# Define Models\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=150, max_depth=10, learning_rate=0.05, \n",
    "                             subsample=0.8, colsample_bytree=0.8, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Train and Evaluate\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Sensitivity and Specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Latency Calculation (Onset Delay)\n",
    "    latency = np.mean([max(0, i - j) for i, j in zip(np.where(y_pred == 1)[0], np.where(y_test == 1)[0])]) if np.any(y_test == 1) else 0\n",
    "\n",
    "    # Save Results\n",
    "    with open(os.path.join(model_results_path, f\"{model_name}_results6.txt\"), \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "        f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "        f.write(f\"Latency: {latency:.4f} samples\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Latency: {latency:.4f} samples\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77f65ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Features: (159783, 144)\n",
      "Total Labels: (159783,)\n",
      "Initial Label Distribution: Counter({np.int64(0): 158785, np.int64(1): 998})\n",
      "\n",
      "Training Set Shape: (127826, 144)\n",
      "Test Set Shape: (31957, 144)\n",
      "Training Labels Distribution: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Labels Distribution: Counter({np.int64(0): 31757, np.int64(1): 200})\n",
      "\n",
      "Balanced Training Labels Distribution: Counter({np.int64(0): 5000, np.int64(1): 798})\n",
      "\n",
      "Training Naive Bayes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Naive Bayes\n",
      "F1 Score: 0.9808\n",
      "Sensitivity (Recall): 1.0000\n",
      "Specificity: 0.9752\n",
      "Latency: 0.1348 seconds\n",
      "Confusion Matrix:\n",
      "[[30971   786]\n",
      " [  102    98]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     31757\n",
      "           1       0.11      0.49      0.18       200\n",
      "\n",
      "    accuracy                           0.97     31957\n",
      "   macro avg       0.55      0.73      0.58     31957\n",
      "weighted avg       0.99      0.97      0.98     31957\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "Model: Random Forest\n",
      "F1 Score: 0.9917\n",
      "Sensitivity (Recall): 1.0000\n",
      "Specificity: 0.9908\n",
      "Latency: 0.1063 seconds\n",
      "Confusion Matrix:\n",
      "[[31466   291]\n",
      " [   36   164]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     31757\n",
      "           1       0.36      0.82      0.50       200\n",
      "\n",
      "    accuracy                           0.99     31957\n",
      "   macro avg       0.68      0.91      0.75     31957\n",
      "weighted avg       0.99      0.99      0.99     31957\n",
      "\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [04:59:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: XGBoost\n",
      "F1 Score: 0.9927\n",
      "Sensitivity (Recall): 1.0000\n",
      "Specificity: 0.9916\n",
      "Latency: 0.0654 seconds\n",
      "Confusion Matrix:\n",
      "[[31489   268]\n",
      " [   20   180]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     31757\n",
      "           1       0.40      0.90      0.56       200\n",
      "\n",
      "    accuracy                           0.99     31957\n",
      "   macro avg       0.70      0.95      0.78     31957\n",
      "weighted avg       1.00      0.99      0.99     31957\n",
      "\n",
      "\n",
      "Model training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Training 7 - Corrected Latency and Balanced Downsampling\n",
    "\n",
    "# Load Features and Labels\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "print(\"\\nTotal Features:\", features.shape)\n",
    "print(\"Total Labels:\", labels.shape)\n",
    "print(\"Initial Label Distribution:\", Counter(labels))\n",
    "\n",
    "# Stratified Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
    "print(\"Test Set Shape:\", X_test.shape)\n",
    "print(\"Training Labels Distribution:\", Counter(y_train))\n",
    "print(\"Test Labels Distribution:\", Counter(y_test))\n",
    "\n",
    "# Downsampling the Majority Class\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy={0: 5000, 1: 798})\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nBalanced Training Labels Distribution:\", Counter(y_train_balanced))\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model Definitions\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# Helper function to calculate latency, sensitivity, and specificity\n",
    "def evaluate_onset_detection(y_true, y_pred, window_size=2, fs=256):\n",
    "    \"\"\"\n",
    "    Calculate latency, sensitivity, and specificity.\n",
    "    \"\"\"\n",
    "    # Convert window indices to time\n",
    "    onset_times_true = np.where(y_true == 1)[0]\n",
    "    onset_times_pred = np.where(y_pred == 1)[0]\n",
    "\n",
    "    # Calculate latency\n",
    "    latencies = []\n",
    "    for true_onset in onset_times_true:\n",
    "        # Find the first correct prediction after the actual onset\n",
    "        valid_preds = onset_times_pred[onset_times_pred >= true_onset]\n",
    "        if len(valid_preds) > 0:\n",
    "            latency = (valid_preds[0] - true_onset) * (window_size / fs)\n",
    "            latencies.append(latency)\n",
    "\n",
    "    # Average latency across all seizure events\n",
    "    avg_latency = np.mean(latencies) if len(latencies) > 0 else float('inf')\n",
    "\n",
    "    # Calculate sensitivity (true positive rate)\n",
    "    sensitivity = len(latencies) / len(onset_times_true) if len(onset_times_true) > 0 else 0\n",
    "\n",
    "    # Calculate specificity (true negative rate)\n",
    "    non_seizure_true = np.sum(y_true == 0)\n",
    "    non_seizure_pred_correct = np.sum((y_pred == 0) & (y_true == 0))\n",
    "    specificity = non_seizure_pred_correct / non_seizure_true if non_seizure_true > 0 else 0\n",
    "\n",
    "    return avg_latency, sensitivity, specificity\n",
    "\n",
    "# Train and Evaluate Models\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Sensitivity, Specificity, and Latency\n",
    "    latency, sensitivity, specificity = evaluate_onset_detection(y_test, y_pred)\n",
    "    \n",
    "    # Save Results\n",
    "    results_file = os.path.join(model_results_path, f\"{model_name}_results7.txt\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        f.write(f\"Model: {model_name}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "        f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "        f.write(f\"Latency: {latency:.4f} seconds\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"Latency: {latency:.4f} seconds\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nModel training and evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fab0854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Features: (159783, 144)\n",
      "Total Labels: (159783,)\n",
      "Initial Label Distribution: Counter({np.int64(0): 158785, np.int64(1): 998})\n",
      "\n",
      "Training Set Shape: (127826, 144)\n",
      "Test Set Shape: (31957, 144)\n",
      "Training Labels Distribution: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Labels Distribution: Counter({np.int64(0): 31757, np.int64(1): 200})\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9937 - loss: 0.0686 - val_accuracy: 0.9966 - val_loss: 0.0114\n",
      "Epoch 2/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9966 - loss: 0.0127 - val_accuracy: 0.9973 - val_loss: 0.0094\n",
      "Epoch 3/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9973 - loss: 0.0109 - val_accuracy: 0.9975 - val_loss: 0.0089\n",
      "Epoch 4/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9977 - loss: 0.0082 - val_accuracy: 0.9978 - val_loss: 0.0082\n",
      "Epoch 5/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0076 - val_accuracy: 0.9977 - val_loss: 0.0078\n",
      "Epoch 6/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9980 - loss: 0.0075 - val_accuracy: 0.9975 - val_loss: 0.0088\n",
      "Epoch 7/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9982 - loss: 0.0069 - val_accuracy: 0.9976 - val_loss: 0.0083\n",
      "Epoch 8/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0061 - val_accuracy: 0.9980 - val_loss: 0.0076\n",
      "Epoch 9/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0059 - val_accuracy: 0.9980 - val_loss: 0.0078\n",
      "Epoch 10/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0056 - val_accuracy: 0.9978 - val_loss: 0.0079\n",
      "Epoch 11/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0053 - val_accuracy: 0.9981 - val_loss: 0.0076\n",
      "Epoch 12/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9987 - loss: 0.0052 - val_accuracy: 0.9981 - val_loss: 0.0082\n",
      "Epoch 13/30\n",
      "\u001b[1m1998/1998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 0.9979 - val_loss: 0.0080\n",
      "\u001b[1m999/999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746us/step\n",
      "\n",
      "F1 Score: 0.8397\n",
      "Sensitivity (Recall): 0.8250\n",
      "Specificity: 0.9991\n",
      "Latency: 814.4974 seconds\n",
      "Confusion Matrix:\n",
      "[[31729    28]\n",
      " [   35   165]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31757\n",
      "           1       0.85      0.82      0.84       200\n",
      "\n",
      "    accuracy                           1.00     31957\n",
      "   macro avg       0.93      0.91      0.92     31957\n",
      "weighted avg       1.00      1.00      1.00     31957\n",
      "\n",
      "\n",
      "LSTM Training and Evaluation Complete!\n"
     ]
    }
   ],
   "source": [
    "#training 8 lstm\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load Features and Labels\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "print(\"\\nTotal Features:\", features.shape)\n",
    "print(\"Total Labels:\", labels.shape)\n",
    "print(\"Initial Label Distribution:\", Counter(labels))\n",
    "\n",
    "# Stratified Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
    "print(\"Test Set Shape:\", X_test.shape)\n",
    "print(\"Training Labels Distribution:\", Counter(y_train))\n",
    "print(\"Test Labels Distribution:\", Counter(y_test))\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for LSTM (samples, time steps, features)\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# LSTM Model Definition\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Metrics\n",
    "precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Sensitivity and Specificity\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Latency Calculation (Samples to Seconds)\n",
    "latency = np.mean([max(0, i - j) for i, j in zip(np.where(y_pred == 1)[0], np.where(y_test == 1)[0])]) * 2\n",
    "\n",
    "# Save Results\n",
    "model_results_path = \"/Users/folasewaabdulsalam/Seizure_Onset/model_results\"\n",
    "os.makedirs(model_results_path, exist_ok=True)\n",
    "results_file = os.path.join(model_results_path, \"LSTM_results.txt\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(f\"F1 Score: {f1_scores:.4f}\\n\")\n",
    "    f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "    f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "    f.write(f\"Latency: {latency:.4f} seconds\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(f\"{cm}\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"\\nF1 Score: {f1_scores:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Latency: {latency:.4f} seconds\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nLSTM Training and Evaluation Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ec785a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Features: (159783, 144)\n",
      "Total Labels: (159783,)\n",
      "Initial Label Distribution: Counter({np.int64(0): 158785, np.int64(1): 998})\n",
      "\n",
      "Training Set Shape: (127826, 144)\n",
      "Test Set Shape: (31957, 144)\n",
      "Training Labels Distribution: Counter({np.int64(0): 127028, np.int64(1): 798})\n",
      "Test Labels Distribution: Counter({np.int64(0): 31757, np.int64(1): 200})\n",
      "\n",
      "Balanced Training Labels Distribution: Counter({np.int64(0): 798, np.int64(1): 798})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "/Users/folasewaabdulsalam/Seizure_Onset/venv/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">53,504</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m53,504\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,105</span> (340.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m87,105\u001b[0m (340.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">86,849</span> (339.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m86,849\u001b[0m (339.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7326 - loss: 0.6275 - val_accuracy: 0.4187 - val_loss: 0.7127\n",
      "Epoch 2/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8643 - loss: 0.3877 - val_accuracy: 0.5469 - val_loss: 0.6463\n",
      "Epoch 3/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8652 - loss: 0.3711 - val_accuracy: 0.6406 - val_loss: 0.5881\n",
      "Epoch 4/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8844 - loss: 0.3111 - val_accuracy: 0.7000 - val_loss: 0.5228\n",
      "Epoch 5/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8693 - loss: 0.3201 - val_accuracy: 0.8031 - val_loss: 0.4521\n",
      "Epoch 6/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9071 - loss: 0.2732 - val_accuracy: 0.8250 - val_loss: 0.3968\n",
      "Epoch 7/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9049 - loss: 0.2452 - val_accuracy: 0.8344 - val_loss: 0.3502\n",
      "Epoch 8/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8731 - loss: 0.3157 - val_accuracy: 0.8375 - val_loss: 0.3185\n",
      "Epoch 9/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8987 - loss: 0.2794 - val_accuracy: 0.8500 - val_loss: 0.2930\n",
      "Epoch 10/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8915 - loss: 0.2997 - val_accuracy: 0.8313 - val_loss: 0.3583\n",
      "Epoch 11/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9227 - loss: 0.2237 - val_accuracy: 0.8719 - val_loss: 0.2479\n",
      "Epoch 12/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9159 - loss: 0.2497 - val_accuracy: 0.8594 - val_loss: 0.3015\n",
      "Epoch 13/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9056 - loss: 0.2612 - val_accuracy: 0.8719 - val_loss: 0.2430\n",
      "Epoch 14/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9225 - loss: 0.2154 - val_accuracy: 0.8687 - val_loss: 0.3030\n",
      "Epoch 15/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9134 - loss: 0.2187 - val_accuracy: 0.8875 - val_loss: 0.2249\n",
      "Epoch 16/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9199 - loss: 0.2296 - val_accuracy: 0.8750 - val_loss: 0.2806\n",
      "Epoch 17/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9231 - loss: 0.2057 - val_accuracy: 0.8719 - val_loss: 0.2913\n",
      "Epoch 18/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9181 - loss: 0.2215 - val_accuracy: 0.8719 - val_loss: 0.2675\n",
      "Epoch 19/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9245 - loss: 0.2231 - val_accuracy: 0.9031 - val_loss: 0.2245\n",
      "Epoch 20/20\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9144 - loss: 0.2292 - val_accuracy: 0.8562 - val_loss: 0.3002\n",
      "\u001b[1m999/999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 586us/step\n",
      "\n",
      "LSTM Model Results\n",
      "F1 Score: 0.9844\n",
      "Sensitivity (Recall): 0.8650\n",
      "Specificity: 0.9783\n",
      "Latency: 4.6650 seconds\n",
      "Confusion Matrix:\n",
      "[[31067   690]\n",
      " [   27   173]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     31757\n",
      "           1       0.20      0.86      0.33       200\n",
      "\n",
      "    accuracy                           0.98     31957\n",
      "   macro avg       0.60      0.92      0.66     31957\n",
      "weighted avg       0.99      0.98      0.98     31957\n",
      "\n",
      "\n",
      "LSTM training and evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Load Features and Labels\n",
    "features, labels = load_features_and_labels(feature_path, annotations_file)\n",
    "\n",
    "print(\"\\nTotal Features:\", features.shape)\n",
    "print(\"Total Labels:\", labels.shape)\n",
    "print(\"Initial Label Distribution:\", Counter(labels))\n",
    "\n",
    "# Stratified Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
    "print(\"Test Set Shape:\", X_test.shape)\n",
    "print(\"Training Labels Distribution:\", Counter(y_train))\n",
    "print(\"Test Labels Distribution:\", Counter(y_test))\n",
    "\n",
    "# Downsampling the Majority Class\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_balanced, y_train_balanced = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"\\nBalanced Training Labels Distribution:\", Counter(y_train_balanced))\n",
    "\n",
    "# Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "X_train_balanced = X_train_balanced.reshape(-1, 1, X_train_balanced.shape[1])\n",
    "X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
    "\n",
    "# LSTM Model Definition\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(1, 144)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the LSTM Model\n",
    "model.fit(X_train_balanced, y_train_balanced, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Predict on Test Set\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Metrics\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "precision, recall, f1_scores, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Sensitivity, Specificity, and Latency\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Overlapping Window Latency Calculation\n",
    "window_size = 2  # seconds\n",
    "fs = 256  # sampling rate\n",
    "latencies = []\n",
    "onset_times_true = np.where(y_test == 1)[0]\n",
    "onset_times_pred = np.where(y_pred == 1)[0]\n",
    "\n",
    "for true_onset in onset_times_true:\n",
    "    valid_preds = onset_times_pred[onset_times_pred >= true_onset]\n",
    "    if len(valid_preds) > 0:\n",
    "        latency = (valid_preds[0] - true_onset) * (window_size / 2)  # account for 50% overlap\n",
    "        latencies.append(latency)\n",
    "\n",
    "avg_latency = np.mean(latencies) if len(latencies) > 0 else float('inf')\n",
    "\n",
    "# Save Results\n",
    "results_file = os.path.join(model_results_path, \"LSTM_results2.txt\")\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "    f.write(f\"Sensitivity (Recall): {sensitivity:.4f}\\n\")\n",
    "    f.write(f\"Specificity: {specificity:.4f}\\n\")\n",
    "    f.write(f\"Latency: {avg_latency:.4f} seconds\\n\")\n",
    "    f.write(\"Confusion Matrix:\\n\")\n",
    "    f.write(f\"{cm}\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nLSTM Model Results\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"Latency: {avg_latency:.4f} seconds\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nLSTM training and evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf3cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
